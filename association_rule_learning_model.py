# -*- coding: utf-8 -*-
"""Association Rule Learning model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Myi-LfTSXwBs7ruvg9yOni3FWQU_PHIk

# **This is the association rule learning model for retail store transaction dataset**
"""

#importing libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

#importing the dataset
def generate_data(location:str,sample_number=10):
  data=pd.read_csv(location,engine='python')
  head=data.head()
  tail=data.tail()
  sample=data.sample(sample_number)
  description=data.describe()
  columns=data.columns
  info=data.info()
  shape=data.shape
  size=data.size
  return {'data':data,'head':head,'tail':tail,'sample':sample,'description':description,'columns':columns,'info':info,
          'shape':shape,'size':size}

data_set = generate_data('/content/TV Shows - Association Rule Learning.csv')

data=data_set['data']

data.head()

data.columns

#renaming the columns
data.rename(columns={"Unnamed: 4": "Peaky Blinders",
                     "Unnamed: 5": "The Boys",
                     "Unnamed: 6": "The Friens",
                     "Unnamed: 7": "How I met your mother",
                     "Unnamed: 8": "The Office",
                     "Unnamed: 9": "I Zombie",
                     "Unnamed: 10": "Teen Wolf",
                     "Unnamed: 11": "The Vampire Diaries",
                     "Unnamed: 12": "The Originals",
                     "Unnamed: 13": "Two and a half men",
                     "Unnamed: 14": "The Big Bang Theory",
                     "Unnamed: 15": "Brooklyn Nine-Nine",
                     "Unnamed: 16": "Shadow Hunters",
                     "Unnamed: 17": "Chilling Adventures of Sabrina",
                     "Unnamed: 18": "Saint Clara's Diet",
                     "Unnamed: 19": "The Walking Dead",
                     "Unnamed: 20": "Kim's Convenience",
                     "Unnamed: 21": "Dark",
                     "Unnamed: 22": "Hell Bound",
                     "Unnamed: 23": "Money Heist",
                     "Unnamed: 24": "Riverdale",
                     "Unnamed: 25": "Another Life",
                     "Unnamed: 26": "Lost in space",
                     "Unnamed: 27": "The Witcher",
                     "Unnamed: 28": "Squid Game",
                     "Unnamed: 29": "Home Sweet Home",
                     "Unnamed: 30": "Stranger Things",
                     "Unnamed: 31": "Lucifer",},
errors="raise", inplace=True)

data.sample(10)

#we need to project our dataset into a particular format
#this function will project our dataset into the required format
def project(data):
  transactions=[]
  rows,columns=data.shape
  # we need to include one extra value then the number of rows
  for i in range (0,rows):
    transactions.append([str(data.values[i,j]) for j in range(0, columns)])
  #this will give the required format with which we can continue now
  #there will be a total of rows entries each containing the entire rows of the dataset
  return transactions

movies = project(data)

#chechking the new format dataset
for val in movies:
  print(val)

#installing the module
!pip install apyori

#building the apriori model
def apriori(transactions,min_support:float,min_confidence:float=0.8,min_lift:int =3,min_length:int=2,max_length:int=2):
  from apyori import apriori
  rules = apriori(transactions = transactions, min_support = min_support, min_confidence = min_confidence, min_lift = min_lift, min_length = min_length, max_length = max_length)
  return rules
# we need to define the following parametres
#1)min_support:let's say we want to consider only those products that appear in atlest 3 transactions in a say 
#ans since this dataset contains the data of an entire week we will multiplt  3 with 7
# so our min support is (3*7)/Number of entrires in our dataset i.e transactions over the week
#therfore min_support  = (3*21)/7500


#2)min_confidence:Some rule of thumbs (start with 0.8 and then keep decreasing the confidence by dividing with 2
#i.e 0.8,0.4,0.2 and so on)
#3)min_lift: rule of thum start with 3 and then keep on Increasing the value based on your dataset
#4)min_confidence:min numbe of elements we want in our rules
#5)max_confidence max number of elements we want in our rule

len(movies)
#we have data collected from 9689 users we will be targeting those shoes which are watched by atleast 100 users therfore
100/9689

rules=apriori(movies,0.001,0.2,3)
#since there is no proper description therefore going with movies/shows that are watched by atleast 10 user
#therefore 10/9689 ~ 0.001
display_Aprori(rules)

rules=apriori(movies,0.001,0.2,3)
#since there is no proper description therefore going with movies/shows that are watched by atleast 10 user
#therefore 10/9689 ~ 0.001
display_Eclat(rules)

#this funnction will display the rules for the apriori model
#this function takes a rule set as an argument
#this is fora rule set with two elements
def display_Aprori(rules,Rows:int=10):
  results = list(rules)
  if len(results) != 0:
    resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])
  # sorting the results in decreasing order based on the lift
    resultsinDataFrame=resultsinDataFrame.nlargest(n = Rows, columns = 'Lift')
    display(resultsinDataFrame)

#this block is hard coded and is specific for a rule set containig only two elements
def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]
    rhs         = [tuple(result[2][0][1])[0] for result in results]
    supports    = [result[1] for result in results]
    confidences = [result[2][0][2] for result in results]
    lifts       = [result[2][0][3] for result in results]
    return list(zip(lhs, rhs, supports, confidences, lifts))

#this funnction will display the rules for the eclat model
#this function takes a rule set as an argument
#this is also hard coded for a 2 item rule set
def display_Eclat(rules,Rows:int=10):
  results = list(rules)
  if len(results) != 0:
    resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Product 1', 'Product 2', 'Support'])
  # sorting the results in decreasing order based on the lift
    resultsinDataFrame=resultsinDataFrame.nlargest(n = Rows, columns = 'Support')
    display(resultsinDataFrame)

#this block is hard coded and is specific for a rule set containig only two elements
def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]
    rhs         = [tuple(result[2][0][1])[0] for result in results]
    supports    = [result[1] for result in results]
    return list(zip(lhs, rhs, supports))

#installing the library
!pip install fpgrowth-py

#this function deduces the rules based on the FP tree approach
def FP_Growth(transactions,min_support,min_confidence):
  from fpgrowth_py import fpgrowth
  freqItemSet, rules = fpgrowth(transactions, minSupRatio=min_support, minConf=min_confidence)
  return {'frequentItemSets':freqItemSet,'rules':rules}

#now how to find the optimal value for min_support and min_confidence
#trying with a set of differnt values

min_support=[0.001] # item should apper atleast 3,4,5,6 times in transcations done in a day
min_confidence=[0.2]#starting with 0.8 and then decreasing the value each time
rulesFP=[] #this will be a list containing all the rules
valuesFP = [] #this will be coressponding values combination for the rules

for support in min_support:
  for confidence in min_confidence:
    rule = FP_Growth(movies,support,confidence)
    rulesFP.append(rule)
    value = {'min_support':support,'min_confidence':confidence}
    valuesFP.append(value)

#this takes quite a long time to execute so execute at your own risk

